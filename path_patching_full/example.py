"""
Example: Full Path Patching on IOI Task

This script demonstrates the complete 5-step path patching algorithm
to identify causal paths in the Indirect Object Identification (IOI) circuit.

The example:
1. Loads GPT-2 small
2. Generates clean and corrupt IOI examples
3. Tests paths from sender heads (e.g., L9H9) to receiver heads
4. Visualizes which paths carry important information
"""

import torch
from transformer_lens import HookedTransformer
from pathlib import Path

# Import our path_patching_full modules
from utils import set_seed
from path_patching import (
    HeadSpec,
    ReceiverSpec,
    path_patch,
    batch_path_patch,
    get_all_attention_head_receivers,
)
from plotting import save_path_patching_heatmap
from data_loader import (
    load_dataset_for_patching,
    print_pair_examples,
)


# ============================================================================
# Configuration
# ============================================================================

# Model settings
MODEL_NAME = "gpt2-small"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Dataset settings
DATASET_SIZE = "large"  # "small", "medium", or "large"
N_EXAMPLES = 500
SEED = 42

# Path patching settings
SENDER_LAYERS = [9, 10]  # Test these layers as senders
RECEIVER_LAYERS = [10, 11]  # Test these layers as receivers

# Output settings
OUTPUT_DIR = "path_patching_results"


# ============================================================================
# Setup
# ============================================================================

def setup():
    """Initialize model and set seeds."""
    set_seed(SEED)
    
    print(f"Loading model: {MODEL_NAME}")
    model = HookedTransformer.from_pretrained(MODEL_NAME, device=DEVICE)
    
    # Create output directory
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    
    return model


# ============================================================================
# Load Pre-generated Dataset
# ============================================================================

def load_ioi_data(model: HookedTransformer):
    """
    Load pre-generated IOI pairs from data_generation_simple.
    
    This uses the dataset generated by data_generation_simple/generate_ioi_pairs.py,
    which creates clean/corrupt pairs with random names (not name-swapping).
    
    Clean: "When John and Mary went to the store, Mary gave a drink to"
           Expected next token: John (IO)
    
    Corrupt: "When Alice and Bob went to the store, Bob gave a drink to"
             (Different names, same structure)
    
    Returns:
        Tuple of (clean_tokens, corrupt_tokens, io_toks, s_toks, pairs)
    """
    print(f"\nLoading IOI dataset (size={DATASET_SIZE}, n_examples={N_EXAMPLES})...")
    
    # Load dataset using data_loader
    clean_tokens, corrupt_tokens, io_toks, s_toks, pairs = load_dataset_for_patching(
        model=model,
        size=DATASET_SIZE,
        n_examples=N_EXAMPLES,
    )
    
    # Print examples
    print_pair_examples(pairs, n=2)
    
    return clean_tokens, corrupt_tokens, io_toks, s_toks, pairs


# ============================================================================
# Baseline Measurements
# ============================================================================

def measure_baselines(model, clean_tokens, corrupt_tokens, io_toks, s_toks):
    """
    Measure baseline logit differences.
    
    These baselines help us interpret path patching results:
    - Clean: Expected to be positive (model predicts IO)
    - Corrupt: Expected to be lower (model uncertain)
    """
    print("\n" + "="*70)
    print("BASELINE MEASUREMENTS")
    print("="*70)
    
    # Compute clean logit diff
    with torch.inference_mode():
        clean_logits = model(clean_tokens)[:, -1, :]
        batch_idx = torch.arange(len(io_toks), device=model.cfg.device)
        clean_io_logits = clean_logits[batch_idx, io_toks]
        clean_s_logits = clean_logits[batch_idx, s_toks]
        clean_logit_diff = (clean_io_logits - clean_s_logits).mean().item()
    
    # Compute corrupt logit diff
    with torch.inference_mode():
        corrupt_logits = model(corrupt_tokens)[:, -1, :]
        # Note: We use clean's IO/S tokens for comparison
        corrupt_io_logits = corrupt_logits[batch_idx, io_toks]
        corrupt_s_logits = corrupt_logits[batch_idx, s_toks]
        corrupt_logit_diff = (corrupt_io_logits - corrupt_s_logits).mean().item()
    
    print(f"\nBaseline logit_diff (IO - S):")
    print(f"  Clean:   {clean_logit_diff:.3f}")
    print(f"  Corrupt: {corrupt_logit_diff:.3f}")
    print(f"  Diff:    {clean_logit_diff - corrupt_logit_diff:.3f}")
    
    # Calculate accuracy on clean
    clean_preds = clean_logits.argmax(dim=-1)
    clean_acc = (clean_preds == io_toks).float().mean().item()
    print(f"  Clean accuracy: {clean_acc:.3f}")
    
    return clean_logit_diff, corrupt_logit_diff


# ============================================================================
# Example 1: Single Path Test
# ============================================================================

def example_single_path(model, clean_tokens, corrupt_tokens, io_toks, s_toks):
    """
    Test a single path: from L9H9 (name mover) to L10H0.
    
    This demonstrates the basic path_patch function.
    """
    print("\n" + "="*70)
    print("EXAMPLE 1: Single Path Testing")
    print("="*70)
    
    # Define sender: Layer 9, Head 9 (known name mover head)
    sender = HeadSpec(layer=9, head=9)
    
    # Define receiver: Layer 10, Head 0, query input
    receiver = ReceiverSpec(layer=10, head=0, component='q')
    
    print(f"\nTesting path: {sender} → {receiver}")
    print("This measures information flow from L9H9 to L10H0's query input")
    
    # Run path patching
    logit_diff = path_patch(
        model=model,
        tokens_origin=clean_tokens,
        tokens_new=corrupt_tokens,
        sender_head=sender,
        receivers=[receiver],
        io_toks=io_toks,
        s_toks=s_toks,
    )
    
    print(f"\nResult: logit_diff = {logit_diff:.3f}")
    print("(Higher values indicate stronger causal influence)")


# ============================================================================
# Example 2: Batch Path Testing with Heatmap
# ============================================================================

def example_batch_paths(model, clean_tokens, corrupt_tokens, io_toks, s_toks):
    """
    Test multiple paths and create a heatmap.
    
    This tests all combinations of sender heads (from specified layers)
    to receiver heads (from specified layers).
    """
    print("\n" + "="*70)
    print("EXAMPLE 2: Batch Path Testing")
    print("="*70)
    
    # Define sender heads to test
    sender_heads = []
    for layer in SENDER_LAYERS:
        for head in range(model.cfg.n_heads):
            sender_heads.append(HeadSpec(layer=layer, head=head))
    
    print(f"\nTesting {len(sender_heads)} sender heads from layers {SENDER_LAYERS}")
    
    # Define receiver heads to test (query inputs)
    receiver_heads = []
    for layer in RECEIVER_LAYERS:
        for head in range(model.cfg.n_heads):
            receiver_heads.append(ReceiverSpec(layer=layer, head=head, component='q'))
    
    print(f"Testing {len(receiver_heads)} receiver heads (Q inputs) from layers {RECEIVER_LAYERS}")
    
    # Run batch path patching
    print("\nRunning path patching (this may take a minute)...")
    effects = batch_path_patch(
        model=model,
        tokens_origin=clean_tokens,
        tokens_new=corrupt_tokens,
        sender_heads=sender_heads,
        receivers=receiver_heads,
        io_toks=io_toks,
        s_toks=s_toks,
    )
    
    print(f"\nPath patching complete!")
    print(f"Effects shape: {effects.shape}")
    print(f"Mean effect: {effects.mean().item():.3f}")
    print(f"Max effect: {effects.max().item():.3f}")
    print(f"Min effect: {effects.min().item():.3f}")
    
    # Find strongest path
    max_idx = effects.argmax()
    max_sender_idx = max_idx // len(receiver_heads)
    max_receiver_idx = max_idx % len(receiver_heads)
    
    strongest_sender = sender_heads[max_sender_idx]
    strongest_receiver = receiver_heads[max_receiver_idx]
    
    print(f"\nStrongest path:")
    print(f"  {strongest_sender} → {strongest_receiver}")
    print(f"  Effect: {effects[max_sender_idx, max_receiver_idx].item():.3f}")
    
    # Save heatmap
    save_path_patching_heatmap(
        effects=effects,
        title=f"Path Patching: Layers {SENDER_LAYERS} → Layers {RECEIVER_LAYERS}",
        filename="path_patching_heatmap.png",
        output_dir=OUTPUT_DIR,
        xlabel=f"Receiver Head (Layers {RECEIVER_LAYERS})",
        ylabel=f"Sender Head (Layers {SENDER_LAYERS})",
    )


# ============================================================================
# Example 3: Testing Different Receiver Components
# ============================================================================

def example_receiver_components(model, clean_tokens, corrupt_tokens, io_toks, s_toks):
    """
    Compare paths to different receiver components (Q, K, V).
    
    This shows how information flows to different parts of attention heads.
    """
    print("\n" + "="*70)
    print("EXAMPLE 3: Receiver Component Comparison")
    print("="*70)
    
    # Define a known important sender
    sender = HeadSpec(layer=9, head=9)
    
    # Test receiver at layer 10, head 0
    receiver_layer = 10
    receiver_head = 0
    
    print(f"\nTesting {sender} to L{receiver_layer}H{receiver_head}")
    print("Comparing information flow to Q, K, and V inputs")
    
    components = ['q', 'k', 'v']
    results = {}
    
    for comp in components:
        receiver = ReceiverSpec(layer=receiver_layer, head=receiver_head, component=comp)
        
        logit_diff = path_patch(
            model=model,
            tokens_origin=clean_tokens,
            tokens_new=corrupt_tokens,
            sender_head=sender,
            receivers=[receiver],
            io_toks=io_toks,
            s_toks=s_toks,
        )
        
        results[comp] = logit_diff
        print(f"  {comp.upper()}: {logit_diff:.3f}")
    
    # Find most important component
    max_comp = max(results, key=results.get)
    print(f"\nMost important component: {max_comp.upper()} ({results[max_comp]:.3f})")


# ============================================================================
# Example 4: Direct Effect Analysis
# ============================================================================

def example_direct_effects(model, clean_tokens, io_toks, s_toks):
    """
    Measure the direct effect of each attention head (similar to Figure 3b).
    
    Direct effect = how much does logit_diff decrease when we ablate this head?
    """
    print("\n" + "="*70)
    print("EXAMPLE 4: Direct Effect Analysis (Figure 3b)")
    print("="*70)
    
    print("\nMeasuring: logit_diff(normal) - logit_diff(head ablated)")
    print("Positive values = head helps predict IO")
    print("Negative values = head works against IOI")
    
    n_layers = model.cfg.n_layers
    n_heads = model.cfg.n_heads
    device = model.cfg.device
    
    # Compute baseline
    with torch.inference_mode():
        baseline_logits = model(clean_tokens)[:, -1, :]
        batch_idx = torch.arange(len(io_toks), device=device)
        baseline_io = baseline_logits[batch_idx, io_toks]
        baseline_s = baseline_logits[batch_idx, s_toks]
        baseline_diff = (baseline_io - baseline_s).mean().item()
    
    print(f"\nBaseline logit_diff: {baseline_diff:.3f}")
    
    # Compute direct effects for all heads
    effects = torch.zeros(n_layers, n_heads)
    
    print(f"\nComputing direct effects for {n_layers} layers × {n_heads} heads...")
    for layer in range(n_layers):
        print(f"  Layer {layer}...", end=" ", flush=True)
        
        for head in range(n_heads):
            # Ablate this head
            def ablate_head_hook(activation, hook):
                activation[:, :, head, :] = 0.0
                return activation
            
            hook_name = f"blocks.{layer}.attn.hook_z"
            
            with torch.inference_mode():
                ablated_logits = model.run_with_hooks(
                    clean_tokens,
                    fwd_hooks=[(hook_name, ablate_head_hook)]
                )[:, -1, :]
                
                ablated_io = ablated_logits[batch_idx, io_toks]
                ablated_s = ablated_logits[batch_idx, s_toks]
                ablated_diff = (ablated_io - ablated_s).mean().item()
            
            # Direct effect = loss from ablation
            effects[layer, head] = baseline_diff - ablated_diff
        
        print("✓")
    
    # Print top heads
    flat_effects = effects.flatten()
    flat_indices = flat_effects.argsort(descending=True)
    
    print(f"\nTop 5 most important heads:")
    for i in range(5):
        idx = flat_indices[i].item()
        layer = idx // n_heads
        head = idx % n_heads
        effect = flat_effects[idx].item()
        print(f"  {i+1}. L{layer}H{head}: {effect:.4f}")
    
    # Save heatmap
    from plotting import save_heatmap
    save_heatmap(
        data=effects,
        title="Direct Effect of Attention Heads on IOI\n(Positive = helps predict IO)",
        filename="direct_effect_heatmap.png",
        output_dir=OUTPUT_DIR,
        xlabel="Head",
        ylabel="Layer",
        colorbar_label="Direct Effect (Δ logit_diff)",
        cmap="RdBu_r",
        figsize=(12, 8),
    )
    
    print(f"\nSaved heatmap to: {OUTPUT_DIR}/direct_effect_heatmap.png")


# ============================================================================
# Main
# ============================================================================

def main():
    """Run all examples."""
    print("="*70)
    print("FULL PATH PATCHING EXAMPLE")
    print("Using pre-generated dataset from data_generation_simple/")
    print("="*70)
    
    # Setup
    model = setup()
    
    # Load pre-generated data
    clean_tokens, corrupt_tokens, io_toks, s_toks, pairs = load_ioi_data(model)
    
    # Measure baselines
    clean_ld, corrupt_ld = measure_baselines(
        model, clean_tokens, corrupt_tokens, io_toks, s_toks
    )
    
    # Run examples
    # example_single_path(model, clean_tokens, corrupt_tokens, io_toks, s_toks)
    # example_batch_paths(model, clean_tokens, corrupt_tokens, io_toks, s_toks)
    # example_receiver_components(model, clean_tokens, corrupt_tokens, io_toks, s_toks)
    
    example_direct_effects(model, clean_tokens, io_toks, s_toks)
    
    
    print("\n" + "="*70)
    print("DONE!")
    print("="*70)
    print(f"\nResults saved to: {OUTPUT_DIR}/")


if __name__ == "__main__":
    main()
